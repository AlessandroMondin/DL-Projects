{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468de957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import inspect\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import torch.optim as optim\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47ffdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NO_PUNCT/DESCRIPTION.json\",\"r\") as f:\n",
    "    preproc_descr = json.load(f)\n",
    "with open(\"SHARED/PROJECT_A.json\",\"r\") as t:\n",
    "    tickets = json.load(t)\n",
    "with open(\"NO_PUNCT/SUMMARIES.json\",\"r\") as s:\n",
    "    summaries = json.load(s)\n",
    "assert len(preproc_descr) == len(tickets) == len(summaries), \"Wrong dimensions\"\n",
    "\n",
    "# joining summary and description into a single list\n",
    "for i in range(len(summaries)):\n",
    "    summaries[i].extend(preproc_descr[i])\n",
    "    \n",
    "df = pd.DataFrame(tickets, columns=[\"ID\",\"bug_cat\",\"labels\",\"components\",\"priority\",\"summary\",\"description\"])\n",
    "descr_ = pd.Series(summaries)\n",
    "df[\"preproc_descr\"] = descr_\n",
    "df.drop([\"description\",\"summary\",\"labels\",\"components\",\"priority\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf31c4",
   "metadata": {},
   "source": [
    "#### 1) data augmentation 2)padding/clipping 4)pre-modeling 5)modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c6980",
   "metadata": {},
   "source": [
    "##### Why not oversampling? -->EASY DATA AUGMENTATION is not that easy: I noticed that replacing  random words with their synonims using wordnet.sysnet generated meaningless sentences and nlpaug tokenizer does not work correctly since neither stopwords nor stopwords_regex as today (02/2022). Back Translation would cost me 200 â‚¬ ......hence undersampling :-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717e64aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_mainclass = []\n",
    "idx_dropclass = []\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i,\"bug_cat\"] == \"YOUR_FIRST_CATEGORY\":\n",
    "        idx_mainclass.append(i)\n",
    "    elif df.loc[i,\"bug_cat\"] == \"YOUR_SECOND_CATEGORY\":\n",
    "        idx_dropclass.append(i)\n",
    "idx_drop = list(np.random.choice(idx_mainclass, int(len(idx_mainclass)*0.4), replace=False))\n",
    "idx_drop.extend(idx_dropclass)\n",
    "df = df.drop(labels=idx_drop, axis=0).reset_index().drop(\"index\",axis=1)\n",
    "lengths = []\n",
    "for i in df[\"preproc_descr\"]:\n",
    "    lengths.append(len(i))\n",
    "df[\"length\"] = lengths\n",
    "labels = df[\"bug_cat\"].tolist()\n",
    "tickets = df[\"preproc_descr\"].tolist()\n",
    "lengths = df[\"length\"].tolist()\n",
    "assert len(labels) == len(tickets) == len(lengths), \"Dimensions are wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75106b6a",
   "metadata": {},
   "source": [
    "#### clipping the lenght of tickets after the 99 percentile to remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7758de77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.percentile(lengths,99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "848a2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(np.percentile(lengths,99))\n",
    "#threshold = 200\n",
    "for i in range(len(tickets)):\n",
    "    if lengths[i] > threshold:       \n",
    "        text = tickets[i]\n",
    "        tickets[i] = text[:threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76dd3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# mindblowing but sexy\n",
    "concat_list = [j for i in tickets for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84e38dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20502 -- 4990\n"
     ]
    }
   ],
   "source": [
    "def vocab_to_int(text,threshold):\n",
    "    counts = dict(Counter(text))\n",
    "    counts = dict(sorted(counts.items(), key=lambda x: x[1],reverse=True))\n",
    "    filtered = [word for word in counts.keys() if counts[word]>=threshold]\n",
    "    new_list = {word:ii for ii, word in enumerate(filtered,1)}\n",
    "    return new_list\n",
    "vocab_2_int =vocab_to_int(concat_list,5)\n",
    "print(len(Counter(concat_list)),\"--\",len(vocab_2_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0aa76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tickets = []\n",
    "for ticket in tickets:\n",
    "    new_ticket = [vocab_2_int[word] for word in ticket if word in vocab_2_int.keys()]\n",
    "    num_tickets.append(new_ticket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1774581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght_clipped = []\n",
    "for i in num_tickets:\n",
    "    lenght_clipped.append(len(i))\n",
    "assert max(lenght_clipped) == threshold, \"Clipping did not work\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757602de",
   "metadata": {},
   "source": [
    "### PADDING --> all lists will have length == threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c0155fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLYING PRE-PADDING THAT WORKS BETTER THAN POST-PADDING\n",
    "def pad_sequence(text,threshold):\n",
    "    if len(text) < threshold:\n",
    "        text = list(np.zeros(threshold-len(text))) + text\n",
    "    return text\n",
    "    \n",
    "num_tickets = [pad_sequence(num_tickets[i],threshold) for i in range(len(num_tickets))]\n",
    "\n",
    "lengths = [len(i) for i in num_tickets]\n",
    "assert sum(lengths)/len(lengths) == max(lengths), \"PADDING DID NOT WORK CORRECTLY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "109fec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling = [[num_tickets[i],labels[i]] for i in range(len(num_tickets))]\n",
    "for i in modeling:\n",
    "    if i[1] == \"YOUR_FIRST_CATEGORY\":\n",
    "        i[1] = 1\n",
    "    else:\n",
    "        i[1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0026a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"NO_PUNCT/modeling.json\",\"w\") as f:\n",
    "#    json.dump(modeling,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0707df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NO_PUNCT/modeling.json\",\"r\") as f:\n",
    "    tickets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5ddd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e45998",
   "metadata": {},
   "source": [
    "#### training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e54c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([x[0] for x in tickets], dtype=int)\n",
    "labels = np.array([x[1] for x in tickets],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e44531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(7677, 243) \n",
      "Validation set: \t(960, 243) \n",
      "Test set: \t\t(960, 243)\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(len(tickets), size=len(tickets), replace=False)\n",
    "perc_tr = 0.8\n",
    "\n",
    "data = data[idx]\n",
    "#data = tickets\n",
    "labels = labels[idx]\n",
    "\n",
    "end_idx_tr = int(len(data)*perc_tr)\n",
    "train_x = data[:end_idx_tr]\n",
    "train_y = labels[:end_idx_tr]\n",
    "\n",
    "remaining = (len(data)-len(train_x))//2\n",
    "\n",
    "\n",
    "val_x = data[end_idx_tr:end_idx_tr+remaining]\n",
    "val_y = labels[end_idx_tr:end_idx_tr+remaining]\n",
    "\n",
    "\n",
    "test_x = data[end_idx_tr+remaining:]\n",
    "test_y = labels[end_idx_tr:end_idx_tr+remaining]\n",
    "\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a52b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, seq_length):\n",
    "    assert len(arr)%seq_length==0, \"Arr must be divisible for seq_length\"\n",
    "    batch_size = len(arr)//seq_length\n",
    "    arr = arr[:seq_length*batch_size]\n",
    "    arr = arr.reshape((batch_size,seq_length))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d8d74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
    "val_data = TensorDataset(torch.from_numpy(val_x),torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x),torch.from_numpy(test_y))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "data_, label = next(iter(train_loader))\n",
    "seq_length = data_.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f6dcd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "933664c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,output_size, embed_size,\n",
    "                 hidden_dim, n_layers,pretrained_embeddings=None, drop_prob=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #self.seq_length = seq_length\n",
    "        # embedding and LSTM layers\n",
    "        if pretrained_embeddings is None:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        print(self.embedding)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # linear and sigmoid layers\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_size) \n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc_out(out)\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:, -1] \n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # if classification were multi-class, \n",
    "        #I should have reshaped (batch_size,self.seq_length, -1)\n",
    "        # ---> sig_out = sig_out.view(batch_size,self.seq_length, -1)\n",
    "        # ---> sig_out = sig_out[:, -1] \n",
    "        \n",
    "        # --> since it's binary, (batch_size,-1) and out = out[:, -1]\n",
    "        # retrieves the second element of each couple out for the last out of the seq_length \n",
    "        #sig_out = sig_out[:, -1] \n",
    "        \n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        # hidden is a tuple since one is h0 and one i c0 (hidden state and cell state)\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b9672",
   "metadata": {},
   "source": [
    "###### I create this class but being my case a many-to-one NLP model, BLTM would be  useless since I am still considering only the last seq_length output in the loss funtion --> I am not using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab124d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(torch.nn.Module): \n",
    "    def __init__(self, vocab_size,output_size, embed_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(BLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.b_lstm = nn.LSTM(embed_size, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_size) \n",
    "        #self.fc1 = nn.Linear(hidden_dim, 400)\n",
    "        #self.fc_out = nn.Linear(400, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        #h0 = torch.zeros(2*self.n_layers,batch_size,self.hidden_dim)\n",
    "        #c0 = torch.zeros(2*self.n_layers,batch_size,self.hidden_dim)\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.b_lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc_out(out)\n",
    "        sig_out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] \n",
    "        return sig_out, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        # hidden is a tuple since one is h0 and one i c0 (hidden state and cell state)\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(2*self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(2*self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(2*self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(2*self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe089b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66033d",
   "metadata": {},
   "source": [
    "#### I also tried to create a fancy loss funtion where I am taking all the losses for each  time step          --> deleting the ones corresponding to the 0-padded entries and the summing and  averaging the remaing ones but the improvements were marginal and training time increase  drastically --> the \"#\" rows in this training funtion refer to that unlucky attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6ef7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(net,epochs,criterion,optimizer,train_loader,val_loader,name_file):\n",
    "    val_loss_min = np.inf\n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "    validation_losses = []\n",
    "    training_losses = []\n",
    "    for i in range(1,epochs+1):\n",
    "        # initialize the hidden_state at the beginning of the epoch    \n",
    "        for ii, (data, label) in enumerate(train_loader,1):\n",
    "            if train_on_gpu:\n",
    "                data, label = data.cuda(), label.cuda()\n",
    "            hidden = net.init_hidden(data.shape[0])\n",
    "            label = label.long().cuda()\n",
    "            net.train()\n",
    "            # detach hidden states\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            out, hidden = net.forward(data, hidden)\n",
    "            #sig_out = out_.view(data.size(0),seq_length,-1)\n",
    "            #labels = torch.zeros(data.size(0),seq_length,1)\n",
    "            #for i in range(labels.size(0)): \n",
    "                #labels[i] = label[i]\n",
    "            #print(labels.size(), torch.unique(labels))\n",
    "            #sig_out = sig_out.view(sig_out.size(0)*sig_out.size(1),-1)\n",
    "            #labels = labels.view(labels.size(0)*labels.size(1),-1)\n",
    "            #print(sig_out.size(), labels.size())\n",
    "            #loss_unfold = nn.BCELoss(reduction=\"none\")(sig_out,labels.cuda())\n",
    "            #data_ = data.view(data.size(0)*seq_length,-1)\n",
    "            #c = 0\n",
    "            #for i in range(data_.size(0)):\n",
    "            #    if data_[i] == 0:\n",
    "            #        c += 1\n",
    "            #        loss_unfold[i]=0\n",
    "            #loss = torch.sum(loss_unfold)/(len(loss_unfold)-c)\n",
    "            \n",
    "            loss = criterion(out, label.float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "              \n",
    "            if ii%(len(train_loader)/4)==0:  \n",
    "                net.eval()     \n",
    "                val_losses = []\n",
    "                corrects = 0\n",
    "                total = 0\n",
    "                \n",
    "                for data, label in val_loader:\n",
    "                    #print(data.size())\n",
    "                    val_h = net.init_hidden(data.shape[0])\n",
    "                    if train_on_gpu:\n",
    "                        data, label = data.cuda(), label.cuda()\n",
    "                    \n",
    "                    label = label.long()\n",
    "                    out, val_h = net.forward(data,val_h)\n",
    "                    loss_ = criterion(out,label.float())\n",
    "                    correct = torch.sum(torch.eq(out.round(),label)).item()\n",
    "                    total += len(data)\n",
    "                    corrects += correct\n",
    "                    val_losses.append(loss_.item())\n",
    "            \n",
    "                validation_losses.append(np.mean(val_losses))\n",
    "                training_losses.append(loss.item())\n",
    "\n",
    "            if ii % (len(train_loader)/2) == 0: \n",
    "                if np.mean(val_losses) < val_loss_min:\n",
    "                    torch.save(net.state_dict(), name_file)\n",
    "                    print(\"Saving model -- Validation Loss decreased from {:.4f} to {:.4f}\".format(val_loss_min,\n",
    "                                                                              np.mean(val_losses)))   \n",
    "                    val_loss_min = np.mean(val_losses)\n",
    "                print(\"Epoch: {}, batch: {}/{}, training loss is {:.3f}\".format(i,ii,len(train_loader),loss.item()))\n",
    "                print(\"Epoch: {}, batch: {}/{}, avg validation loss is {:.3f}\".format(i,ii,len(train_loader),np.mean(val_losses)))\n",
    "                print(\"Epoch: {}, batch: {}/{}, validation accuracy is {:.3f}\".format(i,ii,len(train_loader),corrects/total))\n",
    "                print(\"-----\")\n",
    "        \n",
    "            \n",
    "    return net,validation_losses,training_losses  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa48b9ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(5074, 100)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.BCELoss()\n",
    "vocab_size = len(set([i for x in data for i in x])) + 1\n",
    "epochs=8\n",
    "output_size = 1\n",
    "embed_size = 100\n",
    "hidden_dim = 200\n",
    "clip = 0.2\n",
    "n_layers = 2\n",
    "#batch_size = 32\n",
    "net = LSTM(vocab_size,output_size,embed_size,hidden_dim,n_layers,None,drop_prob=0.3)\n",
    "optim = torch.optim.AdamW(net.parameters(),lr=0.0002)\n",
    "name_file = \"NO_PUNCT/MODELS/attempts.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb8182d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model -- Validation Loss decreased from inf to 0.6789026101430257\n",
      "Epoch: 1, batch: 240/480, training loss is 0.673\n",
      "Epoch: 1, batch: 240/480, avg validation loss is 0.679\n",
      "Epoch: 1, batch: 240/480, validation accuracy is 0.579\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6789026101430257 to 0.6723906646172205\n",
      "Epoch: 1, batch: 480/480, training loss is 0.706\n",
      "Epoch: 1, batch: 480/480, avg validation loss is 0.672\n",
      "Epoch: 1, batch: 480/480, validation accuracy is 0.573\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6723906646172205 to 0.666467759013176\n",
      "Epoch: 2, batch: 240/480, training loss is 0.643\n",
      "Epoch: 2, batch: 240/480, avg validation loss is 0.666\n",
      "Epoch: 2, batch: 240/480, validation accuracy is 0.584\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.666467759013176 to 0.6608087370793024\n",
      "Epoch: 2, batch: 480/480, training loss is 0.698\n",
      "Epoch: 2, batch: 480/480, avg validation loss is 0.661\n",
      "Epoch: 2, batch: 480/480, validation accuracy is 0.603\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6608087370793024 to 0.6408733536799749\n",
      "Epoch: 3, batch: 240/480, training loss is 0.557\n",
      "Epoch: 3, batch: 240/480, avg validation loss is 0.641\n",
      "Epoch: 3, batch: 240/480, validation accuracy is 0.629\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6408733536799749 to 0.6370746687054634\n",
      "Epoch: 3, batch: 480/480, training loss is 0.739\n",
      "Epoch: 3, batch: 480/480, avg validation loss is 0.637\n",
      "Epoch: 3, batch: 480/480, validation accuracy is 0.623\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6370746687054634 to 0.6294651255011559\n",
      "Epoch: 4, batch: 240/480, training loss is 0.492\n",
      "Epoch: 4, batch: 240/480, avg validation loss is 0.629\n",
      "Epoch: 4, batch: 240/480, validation accuracy is 0.634\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6294651255011559 to 0.6254088823994001\n",
      "Epoch: 4, batch: 480/480, training loss is 0.712\n",
      "Epoch: 4, batch: 480/480, avg validation loss is 0.625\n",
      "Epoch: 4, batch: 480/480, validation accuracy is 0.645\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6254088823994001 to 0.6219835763176282\n",
      "Epoch: 5, batch: 240/480, training loss is 0.473\n",
      "Epoch: 5, batch: 240/480, avg validation loss is 0.622\n",
      "Epoch: 5, batch: 240/480, validation accuracy is 0.650\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6219835763176282 to 0.6154743820428848\n",
      "Epoch: 5, batch: 480/480, training loss is 0.659\n",
      "Epoch: 5, batch: 480/480, avg validation loss is 0.615\n",
      "Epoch: 5, batch: 480/480, validation accuracy is 0.658\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6154743820428848 to 0.6120099072655042\n",
      "Epoch: 6, batch: 240/480, training loss is 0.440\n",
      "Epoch: 6, batch: 240/480, avg validation loss is 0.612\n",
      "Epoch: 6, batch: 240/480, validation accuracy is 0.670\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6120099072655042 to 0.6107685233155886\n",
      "Epoch: 6, batch: 480/480, training loss is 0.646\n",
      "Epoch: 6, batch: 480/480, avg validation loss is 0.611\n",
      "Epoch: 6, batch: 480/480, validation accuracy is 0.671\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6107685233155886 to 0.6091062292456627\n",
      "Epoch: 7, batch: 240/480, training loss is 0.439\n",
      "Epoch: 7, batch: 240/480, avg validation loss is 0.609\n",
      "Epoch: 7, batch: 240/480, validation accuracy is 0.691\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6091062292456627 to 0.6080813805262247\n",
      "Epoch: 7, batch: 480/480, training loss is 0.644\n",
      "Epoch: 7, batch: 480/480, avg validation loss is 0.608\n",
      "Epoch: 7, batch: 480/480, validation accuracy is 0.671\n",
      "-----\n",
      "Epoch: 8, batch: 240/480, training loss is 0.404\n",
      "Epoch: 8, batch: 240/480, avg validation loss is 0.612\n",
      "Epoch: 8, batch: 240/480, validation accuracy is 0.685\n",
      "-----\n",
      "Epoch: 8, batch: 480/480, training loss is 0.683\n",
      "Epoch: 8, batch: 480/480, avg validation loss is 0.617\n",
      "Epoch: 8, batch: 480/480, validation accuracy is 0.671\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "net_,val_losses,train_loss = train_nn(net,epochs,loss,optim,train_loader,val_loader,name_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a1d30f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(5074, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LSTM(vocab_size,output_size,embed_size,hidden_dim,n_layers)\n",
    "net.cuda().load_state_dict(torch.load('NO_PUNCT/MODELS/attempts.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb7be2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Confusion_Matrix(net, data_loader):\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in data_loader:\n",
    "            val_h = net.init_hidden(data.shape[0])\n",
    "            if train_on_gpu:\n",
    "                data, label = data.cuda(), label.cuda()\n",
    "                out, val_h = net.forward(data,val_h)\n",
    "            y_pred.append(list(out.round().cpu().numpy()))\n",
    "            y_label.append(list(label.cpu().numpy()))\n",
    "    y_pred = [i for j in y_pred for i in j]\n",
    "    y_label = [i for j in y_label for i in j]\n",
    "    conf_mat = confusion_matrix(y_label,y_pred)\n",
    "    sns.heatmap(conf_mat, annot=True, fmt=\"g\", cmap=\"Blues\")\n",
    "    print(classification_report(y_label,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4632c",
   "metadata": {},
   "source": [
    "##### At this point I investigated and I manager of the project confirmed to me that up to 50% of the ticket labeled as 0 were in reality 1. Quite depressing to discover it after time and enegy spent in this project, but still I had the chance to experience on my skin the fairtale of Garbage In Garbage Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d59c130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.48      0.46       427\n",
      "           1       0.55      0.50      0.52       533\n",
      "\n",
      "    accuracy                           0.49       960\n",
      "   macro avg       0.49      0.49      0.49       960\n",
      "weighted avg       0.50      0.49      0.49       960\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Confusion_Matrix(net,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ecd86767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"NO_PUNCT/Validation_Losses_NO_PUNCT\",\"w\") as f:\n",
    "#    json.dump(val_losses,f)\n",
    "#with open(\"NO_PUNCT/Training_Loss_NO_PUNCT\",\"w\") as f:\n",
    "#    json.dump(train_loss,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98585f7",
   "metadata": {},
   "source": [
    "## PRETRAINED EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f43eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "goog_wordvecs = KeyedVectors.load_word2vec_format('EMBEDDINGS/GoogleNews-vectors-negative300.bin', binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "621de899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_2_tensor(keyedvector_word2vec):\n",
    "    weights = torch.FloatTensor(goog_wordvecs.vectors)\n",
    "    oov_size=0\n",
    "    for i in vocab_2_int.keys():\n",
    "        if i not in goog_wordvecs.key_to_index.keys():\n",
    "            oov_size+=1\n",
    "    zeros = torch.rand(oov_size,weights.shape[1])\n",
    "    weights = torch.cat([weights,zeros],0)\n",
    "    weights.requires_grad = True\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d14c10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = embed_2_tensor(goog_wordvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96ecca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(102117, 300)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.BCELoss()\n",
    "vocab_size = len(set([i for x in data for i in x])) + 1\n",
    "epochs= 12\n",
    "output_size = 1\n",
    "embed_size = 300\n",
    "hidden_dim = 128\n",
    "clip = 0.2\n",
    "n_layers = 2\n",
    "batch_size = 16\n",
    "net = LSTM(vocab_size,output_size,embed_size,hidden_dim,n_layers,pretrained_embeddings,drop_prob=0.2)\n",
    "optim = torch.optim.AdamW(net.parameters(),lr=0.0002)\n",
    "name_file = \"NO_PUNCT/MODELS/pretrained.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62195a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model -- Validation Loss decreased from inf to 0.6855\n",
      "Epoch: 1, batch: 240/480, training loss is 0.686\n",
      "Epoch: 1, batch: 240/480, avg validation loss is 0.686\n",
      "Epoch: 1, batch: 240/480, validation accuracy is 0.555\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6855 to 0.6783\n",
      "Epoch: 1, batch: 480/480, training loss is 0.729\n",
      "Epoch: 1, batch: 480/480, avg validation loss is 0.678\n",
      "Epoch: 1, batch: 480/480, validation accuracy is 0.582\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6783 to 0.6750\n",
      "Epoch: 2, batch: 240/480, training loss is 0.671\n",
      "Epoch: 2, batch: 240/480, avg validation loss is 0.675\n",
      "Epoch: 2, batch: 240/480, validation accuracy is 0.592\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6750 to 0.6678\n",
      "Epoch: 2, batch: 480/480, training loss is 0.723\n",
      "Epoch: 2, batch: 480/480, avg validation loss is 0.668\n",
      "Epoch: 2, batch: 480/480, validation accuracy is 0.600\n",
      "-----\n",
      "Epoch: 3, batch: 240/480, training loss is 0.648\n",
      "Epoch: 3, batch: 240/480, avg validation loss is 0.680\n",
      "Epoch: 3, batch: 240/480, validation accuracy is 0.544\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6678 to 0.6637\n",
      "Epoch: 3, batch: 480/480, training loss is 0.705\n",
      "Epoch: 3, batch: 480/480, avg validation loss is 0.664\n",
      "Epoch: 3, batch: 480/480, validation accuracy is 0.600\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6637 to 0.6619\n",
      "Epoch: 4, batch: 240/480, training loss is 0.610\n",
      "Epoch: 4, batch: 240/480, avg validation loss is 0.662\n",
      "Epoch: 4, batch: 240/480, validation accuracy is 0.595\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6619 to 0.6555\n",
      "Epoch: 4, batch: 480/480, training loss is 0.686\n",
      "Epoch: 4, batch: 480/480, avg validation loss is 0.655\n",
      "Epoch: 4, batch: 480/480, validation accuracy is 0.615\n",
      "-----\n",
      "Epoch: 5, batch: 240/480, training loss is 0.537\n",
      "Epoch: 5, batch: 240/480, avg validation loss is 0.660\n",
      "Epoch: 5, batch: 240/480, validation accuracy is 0.607\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6555 to 0.6545\n",
      "Epoch: 5, batch: 480/480, training loss is 0.667\n",
      "Epoch: 5, batch: 480/480, avg validation loss is 0.655\n",
      "Epoch: 5, batch: 480/480, validation accuracy is 0.613\n",
      "-----\n",
      "Epoch: 6, batch: 240/480, training loss is 0.551\n",
      "Epoch: 6, batch: 240/480, avg validation loss is 0.662\n",
      "Epoch: 6, batch: 240/480, validation accuracy is 0.603\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6545 to 0.6496\n",
      "Epoch: 6, batch: 480/480, training loss is 0.671\n",
      "Epoch: 6, batch: 480/480, avg validation loss is 0.650\n",
      "Epoch: 6, batch: 480/480, validation accuracy is 0.616\n",
      "-----\n",
      "Epoch: 7, batch: 240/480, training loss is 0.554\n",
      "Epoch: 7, batch: 240/480, avg validation loss is 0.657\n",
      "Epoch: 7, batch: 240/480, validation accuracy is 0.611\n",
      "-----\n",
      "Epoch: 7, batch: 480/480, training loss is 0.644\n",
      "Epoch: 7, batch: 480/480, avg validation loss is 0.654\n",
      "Epoch: 7, batch: 480/480, validation accuracy is 0.608\n",
      "-----\n",
      "Epoch: 8, batch: 240/480, training loss is 0.516\n",
      "Epoch: 8, batch: 240/480, avg validation loss is 0.651\n",
      "Epoch: 8, batch: 240/480, validation accuracy is 0.607\n",
      "-----\n",
      "Epoch: 8, batch: 480/480, training loss is 0.648\n",
      "Epoch: 8, batch: 480/480, avg validation loss is 0.656\n",
      "Epoch: 8, batch: 480/480, validation accuracy is 0.618\n",
      "-----\n",
      "Epoch: 9, batch: 240/480, training loss is 0.612\n",
      "Epoch: 9, batch: 240/480, avg validation loss is 0.664\n",
      "Epoch: 9, batch: 240/480, validation accuracy is 0.608\n",
      "-----\n",
      "Epoch: 9, batch: 480/480, training loss is 0.631\n",
      "Epoch: 9, batch: 480/480, avg validation loss is 0.650\n",
      "Epoch: 9, batch: 480/480, validation accuracy is 0.608\n",
      "-----\n",
      "Epoch: 10, batch: 240/480, training loss is 0.555\n",
      "Epoch: 10, batch: 240/480, avg validation loss is 0.652\n",
      "Epoch: 10, batch: 240/480, validation accuracy is 0.616\n",
      "-----\n",
      "Epoch: 10, batch: 480/480, training loss is 0.687\n",
      "Epoch: 10, batch: 480/480, avg validation loss is 0.653\n",
      "Epoch: 10, batch: 480/480, validation accuracy is 0.613\n",
      "-----\n",
      "Epoch: 11, batch: 240/480, training loss is 0.487\n",
      "Epoch: 11, batch: 240/480, avg validation loss is 0.662\n",
      "Epoch: 11, batch: 240/480, validation accuracy is 0.600\n",
      "-----\n",
      "Epoch: 11, batch: 480/480, training loss is 0.675\n",
      "Epoch: 11, batch: 480/480, avg validation loss is 0.653\n",
      "Epoch: 11, batch: 480/480, validation accuracy is 0.623\n",
      "-----\n",
      "Saving model -- Validation Loss decreased from 0.6496 to 0.6393\n",
      "Epoch: 12, batch: 240/480, training loss is 0.531\n",
      "Epoch: 12, batch: 240/480, avg validation loss is 0.639\n",
      "Epoch: 12, batch: 240/480, validation accuracy is 0.631\n",
      "-----\n",
      "Epoch: 12, batch: 480/480, training loss is 0.707\n",
      "Epoch: 12, batch: 480/480, avg validation loss is 0.647\n",
      "Epoch: 12, batch: 480/480, validation accuracy is 0.623\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "net_,val_losses,train_loss = train_nn(net,epochs,loss,optim,train_loader,val_loader,name_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3130e7a",
   "metadata": {},
   "source": [
    "#### Before starting this project I knew I would have faced serious troubles due to the sanity of the labels: unluckily a large percentage of input labeled as 0 were in reality 1 but still I needed to put into practice what I've learnt during my Deep Learning Nanodegree and therefore I decided to go on.\n",
    "#### The lessons I've learnt during this project are 1) Neural Network need a lot of data and might not be the best choice with small datasets 2) I experienced on my skin what GIGO truly is (due to labels) and undirecty I've noticed the limit of supervised-learning too: in fact in order to be useful it needs tons of correctly labeled data, a requirement that might be hard to get in real word. A possible solution in this kind of scenarios might be document-embedding and then clustering, but I give you effort to dive into that jungle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7f322cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(102117, 300)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.69      0.61       427\n",
      "           1       0.68      0.53      0.60       533\n",
      "\n",
      "    accuracy                           0.60       960\n",
      "   macro avg       0.61      0.61      0.60       960\n",
      "weighted avg       0.62      0.60      0.60       960\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net_ = LSTM(vocab_size,output_size,embed_size,hidden_dim,n_layers,pretrained_embeddings,drop_prob=0.3)\n",
    "net_.cuda().load_state_dict(torch.load('NO_PUNCT/MODELS/pretrained.pt'))\n",
    "Confusion_Matrix(net,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377f1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
